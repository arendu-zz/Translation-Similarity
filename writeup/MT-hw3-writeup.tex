\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}
\usepackage{setspace} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[round]{natbib}

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Machine Translation Evaluation Assignment}

\author{Adithya Renduchintala \\
  {\tt adithya.renduchintala@jhu.edu}\\
  }

\date{}

\begin{document}
\maketitle
%\begin{abstract}
%  This document contains the instructions for preparing a camera-ready
  % manuscript for the proceedings of ACL2012. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.
%\end{abstract}


\section{Introduction}

In this assignment  outputs of 2 machine translation (MT)
systems were compared against a human generated reference translation. The task
is to automatically detect which one of the 2 MT outputs were closer to the
human generated output. The automatic evaluation was setup as a 3-way
classification task. The 3 possible labels are (a) hypothesis of MT system 1
was closest to the reference, (b) hypothesis of system 2 was closeset to the
reference or (c) both systems were equidistant from the reference.This paper
presents an approach using a Vector Space model and a Language model scores to 
generate training examples for the classification task. An of-the-shelf Support 
Vector Machine (SVM) classifier was used.


\section{Vector Space Model}
Vector Space Models (VSM) has been used extensively for information retrieval
and various NLP tasks (\cite{Manning:2008:IIR:1394399}). The goal of VSM is to
assign a continuous valued vector to each word type in a corpus.\\
\begin{align*}                                                                                                                                                                   
\vec{x} &= \{ x_1, x_2, \ldots, x_n\}
\end{align*}
 This enables the computatioin of mathematical metrics such as distance or
 similarity. This gives us a clear way to compare the 2 machine translation
 hypothesis to the reference. First each of the translations were converted to
 their vector representation. This was done using an of-the-shelf Latent
 Semantic Indexing toolkit (\cite{rehurek_lrec}). The process essentially
 involves constructing a term-document matrix (\cite{turney2010frequency}).
 Singular Value Decomposition (SVD) is applied to this matrix. The resulting singular value matrix is
 truncated and using this truncated SV matrix a truncated (or low-rank)
 term-document matrix can be regenerated. By looking up the vectors associated
 with each term we obtain a set of word vectors. This is known as Latent
 Semantic Analysis (LSA). By looking up vectors for each document we obtain
 document level vector representations, which is known as Latent Semantic
 Indexing (LSI). Both operations, however, involve the same mathematical
 process. Once individual word vectors are obtained, there are several very
 interesting methods to compose word vectors into phrase/sentence vectors,
 (\cite{hermann2013role},\cite{socher2012semantic}), but for this assignment
 simple elemental average was used. Cosine Similarity (CS) was used to measure which of the 2 hypothesis were closer to the reference. CS is robust to sparsity and is very commonly used for IR tasks. Unlike euclidean distance, CS is returns the angular separation of the 2 vectors.
 \begin{align*}
 cos(\vec{a},\vec{b}) &= \frac{\vec{a}\cdot \vec{b}}{\mid\vec{a}\mid
 \mid\vec{b}\mid}
 \end{align*}
 Additionally a nice property of cosine similarity is that the range of values
 produced by the metric is between +1 (strong similarity) and -1 (strong
 opposition). A cosine similarity of 0 means the 2 vectors are orthogonal to
 each other. For our task, this is how the difference in meaning of the
 hypothesis sentences and the reference sentence is modeled.
 \section{Language Model}
 In order to automatically select the better hypothesis, merely using a bag of
 words based vector space model is not enough. The vector model makes no
 structural requirements from hypothesis. To capture how well-formed the 2
 hypothesis are, a simple n-gram language model score was computed. Since the
 data provided was only around 2000 unique references, a larger corpus was used
 to train a trigram language model. The ACL 2013 Shared Task in Machine
 Translation provided a large set of corpora, a portion of the 2012 news crawl
 corpus was used to train our LM. KenLM was used to for training and for scoring a
 sentence (\cite{heafield2011kenlm}). Using the LM scores, Cosine Similarity
 scores and the vector representations themselves training features were created
 which were in turn used to train an SVM classifier. In the next section we will
 discuss the feature engineering used to come up with the training samples for
 classification.
 \section{Feature Engineering}
In this section we will go over a number of features that were created using
outputs form the Language model, a trained vector space model and the meteor
score metric.
\subsection{Meteor Metric}
The Meteor metric is a commonly used MT evaluation baseline
(\cite{koehn2010statistical}).
\begin{align*}
R(hyp,ref) &= \frac{\mid hyp \cap ref}{\mid ref \mid}\\
P(hyp,ref) &= \frac{\mid hyp \cap ref}{\mid hyp \mid}\\
\end{align*}
In our case we compute $R_1 = R(hyp_1, ref), P_1 = P(hyp_1,ref)$ and $R_2 =
R(hyp_2, ref), P_2 = P(hyp_2, ref)$. From this we compute the meteor scores
$M_1,M_2$ for each hypothesis. An $\alpha$ of 0.8 was used as it provided the
best accuracy (on training samples) during baseline test.
\begin{align*}
M_1 &= \frac{R_1 \cdot P_1}{(1-\alpha)R_1 + \alpha P_1}\\
M_2 &= \frac{R_1 \cdot P_1}{(1-\alpha)R_1 + \alpha P_1}\\
m_1 &= \begin{cases} 1 &\mbox{if } M_1 > M_2\\
0\\
\end{cases}\\
m_2 &= \begin{cases} 1 &\mbox{if } M_1 < M_2\\
0\\
\end{cases}\\
m_3 &= \begin{cases} 1 &\mbox{if } M_1 = M_2\\
0\\
\end{cases}\\
\vec{M} &= [m_1, m_2, m_3]\\
\end{align*}
A on/off vector ($\vec{M}$) was created and added to the set of features using
the meteor scores and added to the training instance. 
\subsection{Vector Space Features}
 We also augmented the corpus from which the LSA model was computed,
with a portion of the 2012 news crawl corpus form 2013 ACL shared task in
machine translation.Recall that once the LSA model was constructed for each 
token is associated with a vector $\vec{x}\in \mathbb{R}^n$.
Since we employed simple element-wise averaging we again obtain a vector
$\vec{x_p} \in \mathbb{R}^n$, where $\vec{x_p}$ represents the vector for a bag
of words. The choice of $n$ is arbitrary, a value of 100 was chosen for
our expriments, we also verified that higher values gave slightly better
performance. First, the hypotheses (hyp1, hyp2) and the reference phrases are
tokenized. Often there is a lot of word overlap between the hypotheses, to take
advantage of this we compute the set subtraction of hyp1 from hyp2 and extract
the bag of words vector that represents the result of this subtraction. The same
was done for hyp2 as well. For example if hyp1 is \emph{Looking in the cities}
and hyp2 is \emph{Looking at the towns}, hyp1 is updated to the set of tokens \{
in, cities\} and hyp2 is updated to the set of tokens \{ at, towns\}. These
tokens are used to compute the vectors. Using the vectors the cosine similarity
was from each hypothesis to the reference was computed. The set of equations
below show all of the operations performed to get the vector space features.
Apart from just the cosine scores, we found added on/off features helpful. The
equations for $f_1, f_2$ and $f_3$ show how the on/off features were computed.
\begin{align*}
h_1 &= hyp_1 - hyp_2\\
h_2 &= hyp_2 - hyp_1\\
\vec{x_1} &= \frac{1}{\mid h_1 \mid} \sum \limits_{h\in h_1} vec(h)\\
\vec{x_2} &= \frac{1}{\mid h_2 \mid} \sum \limits_{h\in h_2} vec(h)\\
\vec{r} &= \frac{1}{\mid ref \mid} \sum \limits_{h \in ref} vec(h)\\
cs_1 &= \frac{\vec{x_1}\cdot\vec{r}}{\mid \vec{x_1}\mid \mid \vec{r} \mid}\\
cs_2 &= \frac{\vec{x_2}\cdot\vec{r}}{\mid \vec{x_2}\mid \mid \vec{r} \mid}\\
cs_d &= \mid cs_1 - cs_2 \mid\\
f_1 &= \begin{cases} 1 &\mbox{if } cs_1 > cs_2 \\
0 \\
\end{cases}\\
f_2 &= \begin{cases} 1 &\mbox{if } cs2 > cs 1\\
0\\
\end{cases}\\
f_3 &= \begin{cases} 1 &\mbox{if } cs1 =cs_2\\
0\\
\end{cases}\\
\vec{CS} &= [f_1, f_2, f_3, cs_1, cs_2, cs_d]
\end{align*}
This feature vector does not give the SVM a chance to learn which elements in
the word vectors are important for classification. It is possible that some
elements (by position) in a word vector are more useful for classification. To
incorporate this information, the full vectors $\vec{x_1}, \vec{x_2}$ and
$\vec{r}$ were normalized and added to the training sample. The normalization
ensured that the element values were between 0 and 1. This is expressed below,
the addition operator concatenates the vectors.
\begin{align*}
\vec{FULL} &= norm(\vec{x_1}) +norm(\vec{x_2})+ norm(\vec{r})\\ 
\end{align*}
But even with the $\vec{FULL}$ vector there is no explicit information which
indicates how which elements indexes in the 3 vectors contain similar values. To
add this information, the phrase vectors where binarized using the following
scheme. Let $\vec{x}$ be a normalized phrase vector (either for hyp1, hyp2 or
reference). Each element $x_i \in \vec{x}$ is made 1 if its greater than the
median value in $\vec{x}$ otherwise it is set to 0.
\begin{align*}
x_i &= \begin{cases} 1 &\mbox{if } x_i > median(\vec{x})\\
0\\
\end{cases}
\end{align*}
This binarization scheme was applied to each of the phrase vectors (hyp1, hyp2
and reference). Then an element-wise intersection (or multiplication) was
performed on binarized hypotheses with the binarized reference vectors. The
binarized intersections were concatenated and this was used as features as well.
In the expressions below $bin()$ represents the binarization described above.
\begin{align*}
\vec{BI_1} &= bin(\vec{x_1}) \cap bin(\vec{r})\\
\vec{BI_2} &= bin(\vec{x_2}) \cap bin(\vec{r})\\
\vec{INT} &= \vec{BI_1} + \vec{BI_2}\\
\end{align*}
Thus the total set of features at our disposal from the vector space model are
\begin{enumerate}
  \item M Features: the meteor score based binary vector.
  \item CS Features: The cosine similarity metric scores ($cs_1$,$cs_2$,$cs_d$)
  and the associated indicator elements ($f_1$,$f_2$,$f_3$). There are exactly 6
  elements in this feature set.
  \item FULL Features: These are the 'dump' of the full normalized phrase
  vectors for the hypotheses and the reference. This feature set has $3n$
  elements given that each phrase/word vector is $\mathbb{R}^n$
  \item INT Features: These are the binarized and intersected (with reference)
  features. These feature set has $2n$ elements.
\end{enumerate}
\subsection{Language Model Features}
The M and CS features show similarity of the hypothesis to the reference, while
the LM features encode into the feature vector how well formed the hypotheses
are. KenLM was used to compute this score for each hypotheses and for the
reference. Using the raw LM score could result in a mis-informed classifier.
Consider the case where hyp1 is a one-word translation while hyp2 is a longer
phrase. Even if hyp2 is well formed, the fact that it has more to tokens will
severely reduce its LM score. To counter this, the LM scores were computed per
token. The scale of the LM scores are very different from the VSM features, in
order to restrict the feature scale the following operations were applied.
\begin{align*}
lms_1 &= \frac{score(hyp1)} {\mid hyp1 \mid}\\
lms_2 &= \frac{score(hyp2)} {\mid hyp2 \mid}\\
lms_r &= \frac{score(ref)} {\mid ref \mid}\\
lm_1 &= \frac{lms_r}{lms_1}\\
lm_2 &= \frac{lms_r}{lms_2}\\
lm_d &= \mid lm_1 - lm_2 \mid\\
l_1 &= \begin{cases} 1 &\mbox{if } lm_1 > lm_2\\
0\\
\end{cases}\\
l_2 &= \begin{cases} 1 &\mbox{if } lm_1 < lm_2\\
0\\
\end{cases}\\
l_3 &= \begin{cases} 1 &\mbox{if } lm_1 = lm_2\\
0\\
\end{cases}\\
\vec{LM} &= [l_1, l_2, l_3, lm_1, lm_2, lm_d]\\
\end{align*}
\section{Experiments}
Apart form experimenting with different feature sets, different attributes
within the VSM were also tested. The following sections describe these
properties that were experimented with.
\subsection{TF-IDF Vs Binary Weighting}
Term Frequency-Inverse Document Frequency is a of weighting tokens. The idea
behind tf-idf weighting is that very popular tokens (eg. stop words) would
appear in many documents and should not be weighted as much as seeing a
relatively rare word. This is typically used in IR tasks, where we do not want
to match document to a query based on common words. In this task, we can use the
tfifd weight of a word to scale the word vector. Binary weighting, as the name
suggests would simply scale the word vector by 1 (no scaling).
\subsection{LDA vs LSA}
The Gensim toolkit provides easy interface to generate VSM models, but it also
traings topic models. This allows for an alternate vector representation of the
words. Once a topic model is trained, we can look up the distribution of a word
over the number of topics. This is a multinomial dirichlet than can  easily
substitute the LSA word vector.
\subsection{Initial Results}
In this section, we test all of the feature sets and other tunable attributes on
the first 10,000 data points provided. This was mainly done to speed up turn
around time of the intermediate results. The \cite{scikit-learn} toolkit was used for
classification, an multi-class SVM was used with an RBF kernel. The table below shows the average accuracy after running 3-fold cross validation. These numbers could also be increased by increasing the size of the word vector representation (or number of topics for LDA), but to get fast results we limited the size to 100.
\begin{table}[h]
\begin{center}
\begin{singlespace}
\begin{tabular}{|l|l|l|l|l|}
\hline \bf Features & \bf LSA & \bf TFIDF  & \bf LDA \\ \hline
1.CML &0.432& 0.410 & 0.436\\
2.CMLF &0.580 & 0.597&0.411\\
3.CMLI & 0.620 & 0.629&0.406\\
4.CMLFI&0.616& 0.629&0.37 \\
\hline
\end{tabular}
\end{singlespace}
\end{center}
\caption{ Accuracy on first 10000 training samples with 3-fold cross validation.
  (1) CML = M, CS and LM features, 
  (2) CMLF = M, CS LM and FULL features,
  (3) CMLI = M, CS LM and INT features,
  (4) CMLFI = M, CS LM, FULL and INT}
\end{table}
The results indicate that the biggest gain is provided by the INT features,
which were the binarized intersected features. It also shows that the TF-IDF
weighting scheme does help improve over the simple 1/0 scaling of word vectors.
The accuracy generated by the LDA model is significantly lower than the LSA
model. There could be 2 explanations for this: Firstly, cosine similarity might
not be the best metric to measure the similarity of 2 distributions, to remedy this
KL-divergence was used, however even with KL divergence similar results were
observed. Secondly, it is possible that LDA requires much later training corpus
to generate robust word vectors. To expedite training of these models a subset
of the 2012 news crawl data set was used (around 75000 lines). This is probably
low for LDA.\\
Next, 3-fold cross validation was performed on the full (~25,000) training
samples provided.  The lowest performing
model in the small (10,000 sample) test was not included in the full cross
validation.The following results were observed. It should be noted that these
follow a very similar trend noticed in the smaller test.\\
\begin{table}[h]
\begin{center}
\begin{singlespace}
\begin{tabular}{|l|l|l|l|l|}
\hline \bf Features & \bf LSA & \bf TFIDF  & \bf LDA \\ \hline
2.CMLF &0.582 & 0.600&0.323\\
3.CMLI & 0.599 & 0.605&0.337\\
4.CMLFI&0.605&\bf{0.613} &0.337 \\
\hline
\end{tabular}
\end{singlespace}
\end{center}
\caption{ Accuracy on full 25000 training samples with 3-fold cross validation.}
\end{table}
Since this indicated that the training scales to a larger data set, the winning
model was submitted. However the only scored 0.47 on the unseen test set.
\bibliographystyle{plainnat}
\bibliography{bib}{} 
\end{document}
